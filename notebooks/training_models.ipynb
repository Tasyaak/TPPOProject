{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Текущая рабочая директория Jupyter\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Если CWD = Project/notebooks, то корень проекта = parent\n",
    "project_root = str(cwd.parent)\n",
    "\n",
    "# Добавляем корень проекта в sys.path (в начало, чтобы он имел приоритет)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from config import DB_PATH, NOTEBOOKS_DIR \n",
    "\n",
    "BASE = NOTEBOOKS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# !sudo apt-get install -y git-lfs\n",
    "# !git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "cfg_path = BASE / \"default.yaml\"\n",
    "with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e69596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd\n",
    "\n",
    "sql = cfg[\"data\"][\"sql\"]\n",
    "\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    df = pd.read_sql(sql, conn)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, json\n",
    "\n",
    "\n",
    "def _parse_maybe_list(x : str) -> list:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # JSON list\n",
    "        try:\n",
    "            v = json.loads(s)\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Python literal list (безопаснее, чем eval)\n",
    "        try:\n",
    "            v = ast.literal_eval(s)\n",
    "            if isinstance(v, list):\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "        # fallback: whitespace tokens\n",
    "        return s.split()\n",
    "    # fallback: try iter\n",
    "    try:\n",
    "        return list(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "    \n",
    "def _identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23936369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _tokens_col_to_text(X : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ColumnTransformer отдаёт (n_samples, 1) -> вернём массив строк (n_samples,)\"\"\"\n",
    "    col = np.asarray(X).reshape(-1)\n",
    "    out = [\" \".join(map(str, _parse_maybe_list(v))) for v in col]\n",
    "    return np.asarray(out, dtype=object)\n",
    "\n",
    "def _numeric_col_to_matrix(X : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Парсит ctx_numeric (ожидается фиксированная длина списка чисел)\"\"\"\n",
    "    col = np.asarray(X).reshape(-1)\n",
    "    rows = [np.asarray(_parse_maybe_list(v), dtype=float) for v in col]\n",
    "    # если длины разные — лучше падать явно\n",
    "    lens = {r.shape[0] for r in rows}\n",
    "    if len(lens) != 1:\n",
    "        raise ValueError(f\"ctx_numeric has varying lengths: {sorted(lens)}\")\n",
    "    return np.vstack(rows)\n",
    "\n",
    "\n",
    "PREPROCESSORS = {\n",
    "    \"identity\": _identity,\n",
    "}\n",
    "TOKENIZERS = {\n",
    "    \"identity\": _identity,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fa002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def build_text_tokens(field_cfg : dict, *, lowercase : bool, naive_bayes_compatible : bool) -> Pipeline:\n",
    "    mode = field_cfg[\"mode\"]\n",
    "    ngram_range = tuple(field_cfg[\"ngram_range\"])\n",
    "    token_pattern = field_cfg[\"token_pattern\"]\n",
    "\n",
    "    tokenizer_name = field_cfg[\"tokenizer\"]\n",
    "    preprocessor_name = field_cfg[\"preprocessor\"]\n",
    "    preprocessor = PREPROCESSORS[preprocessor_name]\n",
    "    tokenizer = TOKENIZERS[tokenizer_name]\n",
    "\n",
    "    expects_tokens = (tokenizer_name == \"identity\") or (preprocessor_name == \"identity\") or (token_pattern is None)\n",
    "    to_text = FunctionTransformer(_tokens_col_to_text, validate=False) if expects_tokens else \"passthrough\"\n",
    "\n",
    "    if mode == \"CountVectorizer\":\n",
    "        base_params = dict(field_cfg[\"CountVectorizer\"])\n",
    "        vect = CountVectorizer(\n",
    "            lowercase=lowercase,\n",
    "            ngram_range=ngram_range,\n",
    "            preprocessor=preprocessor,\n",
    "            tokenizer=tokenizer,\n",
    "            token_pattern=token_pattern,\n",
    "            **base_params,\n",
    "        )\n",
    "    elif mode == \"HashingVectorizer\":\n",
    "        base_params = dict(field_cfg[\"HashingVectorizer\"])\n",
    "        if naive_bayes_compatible:\n",
    "            base_params[\"alternate_sign\"] = False  # чтобы не получить отрицательные значения\n",
    "        vect = HashingVectorizer(\n",
    "            lowercase=lowercase,\n",
    "            ngram_range=ngram_range,\n",
    "            preprocessor=preprocessor,\n",
    "            tokenizer=tokenizer,\n",
    "            token_pattern=token_pattern,\n",
    "            **base_params,\n",
    "        )\n",
    "    elif mode == \"TfidfVectorizer\":\n",
    "        base_params = dict(field_cfg[\"TfidfVectorizer\"])\n",
    "        vect = TfidfVectorizer(\n",
    "            lowercase=lowercase,\n",
    "            ngram_range=ngram_range,\n",
    "            preprocessor=preprocessor,\n",
    "            tokenizer=tokenizer,\n",
    "            token_pattern=token_pattern,\n",
    "            **base_params,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    steps = []\n",
    "    if to_text != \"passthrough\":\n",
    "        steps.append((\"to_text\", to_text))\n",
    "    steps.append((\"vect\", vect))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "def build_numeric_dict(field_cfg : dict) -> Pipeline:\n",
    "    mode = field_cfg[\"mode\"]\n",
    "    scale = field_cfg[\"scale\"]\n",
    "\n",
    "    if mode == \"DictVectorizer\":\n",
    "        base_params = dict(field_cfg[\"vectorizers\"][\"DictVectorizer\"])\n",
    "        dv = DictVectorizer(**base_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "    \n",
    "    if scale == \"StandardScaler\":\n",
    "        base_params = dict(field_cfg[\"scaling\"][\"StandardScaler\"])\n",
    "        sc = StandardScaler(**base_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scaling: {scale}\")\n",
    "    \n",
    "    ctx_vec = Pipeline([\n",
    "            (\"select\", FunctionTransformer(lambda s: list(s), validate=False)),  # если подаёте уже list[dict], не нужно\n",
    "            (\"dv\", dv),\n",
    "            (\"scale\", sc),\n",
    "        ])\n",
    "    return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def build_vectorizer_from_cfg(cfg : dict, *, col_map : dict[str, str] | None = None) -> ColumnTransformer:\n",
    "    features = cfg[\"features\"]\n",
    "    lowercase = bool(features[\"lowercase\"])\n",
    "    nb_ok = bool(features[\"naive_bayes_compatible\"])\n",
    "\n",
    "    # дефолтное соответствие: под ваш SQL SELECT\n",
    "    col_map = col_map or {\n",
    "        \"for_ctx_numeric\": \"ctx_numeric\",\n",
    "        \"for_ctx_tokens\": \"ctx_tokens\",\n",
    "        \"for_error_text\": \"error_text_tokens\",\n",
    "    }\n",
    "\n",
    "    tr = []\n",
    "\n",
    "    tr.append((\n",
    "        \"source_code\",\n",
    "        build_text_tokens(features[\"for_ctx_tokens\"], lowercase=lowercase, naive_bayes_compatible=nb_ok),\n",
    "        col_map[\"for_ctx_tokens\"],\n",
    "    ))\n",
    "\n",
    "    tr.append((\n",
    "        \"error_text\",\n",
    "        build_text_tokens(features[\"for_error_text\"], lowercase=lowercase, naive_bayes_compatible=nb_ok),\n",
    "        col_map[\"for_error_text\"],\n",
    "    ))\n",
    "\n",
    "    tr.append((\n",
    "        \"ctx_numeric\",\n",
    "        build_numeric_dict(features[\"for_ctx_numeric\"]),\n",
    "    ))\n",
    "\n",
    "    return ColumnTransformer(transformers=tr, remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, sqlite3, pandas as pd\n",
    "\n",
    "\n",
    "with open(\"configs/default.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "with sqlite3.connect(\"db/app.db\") as conn:\n",
    "    df = pd.read_sql(cfg[\"data\"][\"sql\"], conn)\n",
    "\n",
    "y = df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "feat = build_vectorizer_from_cfg(cfg)\n",
    "X = feat.fit_transform(df)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
