{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a82b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, math, json, pandas as pd, pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "# Текущая рабочая директория Jupyter\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Если CWD = Project/notebooks, то корень проекта = parent\n",
    "project_root = cwd.parent\n",
    "\n",
    "# Добавляем корень проекта в sys.path (в начало, чтобы он имел приоритет)\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from processing_cpp import compile_get_error_info, clear_build_tmp, safe_extract_context\n",
    "from config import TEMP_OUTPUT_DIR, PARQUETS_DIR, CTX_JSONLS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../errors_cpp_codes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df[70000:71000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ad5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def bulk_get_error_text_batch(codes: list[str], max_workers: int = 4) -> list[tuple[str | None, int | None]]:\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        return list(ex.map(compile_get_error_info, codes))\n",
    "\n",
    "def fill_error_text_in_batches(df : pd.DataFrame, batch_size : int = 500, max_workers : int = 4) -> pd.DataFrame:\n",
    "    if \"error_text\" not in df.columns:\n",
    "        df[\"error_text\"] = None\n",
    "    if \"error_line\" not in df.columns:\n",
    "        df[\"error_line\"] = pd.Series([None] * len(df), dtype=\"Int64\")\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return df\n",
    "    \n",
    "    for start in range(0, n, batch_size):\n",
    "        stop = min(start + batch_size, n)\n",
    "        batch_index = df.index[start:stop]\n",
    "\n",
    "        codes = df.loc[batch_index, \"source_code\"].tolist()\n",
    "\n",
    "        results = bulk_get_error_text_batch(codes, max_workers=max_workers)\n",
    "        error_texts = [r[0] for r in results]\n",
    "        error_lines = [r[1] for r in results]\n",
    "        df.loc[batch_index, \"error_text\"] = error_texts\n",
    "        df.loc[batch_index, \"error_line\"] = error_lines\n",
    "\n",
    "        print(f\"{stop}/{n}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = fill_error_text_in_batches(df_slice, batch_size=500, max_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_build_tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8109d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.width\", 2000)     # максимальная ширина в символах\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = {\n",
    "    'C2065',\n",
    "    'C3861',\n",
    "    'C2143',\n",
    "    'C2146',\n",
    "    'C2059',\n",
    "    'C1075',\n",
    "    'C1083',\n",
    "    'C2131',\n",
    "    'C2440',\n",
    "    'C2446',\n",
    "    'C2676',\n",
    "    'C2678',\n",
    "    'C2679',\n",
    "    'C2039',\n",
    "    'C2672',\n",
    "    'C2144',\n",
    "    'C2187',\n",
    "    'C2148',\n",
    "    'C2064',\n",
    "    'C2181',\n",
    "    'C2106',\n",
    "}\n",
    "\n",
    "pattern = '|'.join(codes)\n",
    "msk = df_correct[\"error_text\"].str.contains(pattern, regex=True)\n",
    "df_correct = df_correct[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7503e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "need = pd.read_parquet(PARQUETS_DIR / \"data_filtered_error_codes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5437f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225693"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9f85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk2 = need[\"source_code\"].str.contains(r'#\\s*(?:define|undef|if|ifdef|ifndef|elif|pragma)', regex=True)\n",
    "msk3 = need[\"error_code\"] == \"C1083\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c583ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need_macros = need[msk2 | msk3]\n",
    "need_without_macros = need[~msk2 & ~msk3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7cf91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154276"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(need_without_macros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c68421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_nones(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        cleaned = {}\n",
    "        for k, v in obj.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            v_clean = strip_nones(v)\n",
    "            if isinstance(v_clean, (dict, list)) and not v_clean:\n",
    "                continue\n",
    "            cleaned[k] = v_clean\n",
    "        return cleaned\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        cleaned_list = []\n",
    "        for v in obj:\n",
    "            if v is None:\n",
    "                continue\n",
    "            v_clean = strip_nones(v)\n",
    "            if isinstance(v_clean, (dict, list)) and not v_clean:\n",
    "                continue\n",
    "            cleaned_list.append(v_clean)\n",
    "        return cleaned_list\n",
    "    return obj\n",
    "\n",
    "def clean_jsonl(in_path : str | Path, out_path : str | Path) -> None:\n",
    "    in_path = Path(in_path)\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    with in_path.open(\"r\", encoding=\"utf-8\") as fin, out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            obj = json.loads(line)\n",
    "            ctx = obj.get(\"ctx\")\n",
    "            if ctx:\n",
    "                clean_ctx = strip_nones(ctx)\n",
    "                obj[\"ctx\"] = clean_ctx\n",
    "\n",
    "            fout.write(json.dumps(obj, ensure_ascii=False))\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_jsonl(CTX_JSONLS_DIR / \"third_part_without_macros.jsonl\", CTX_JSONLS_DIR / \"third_part_without_macros_clean.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ctx_parquet_to_jsonl(src_path: str, out_jsonl: str, batch_size: int = 2000) -> None:\n",
    "    pf = pq.ParquetFile(src_path)\n",
    "    total_rows = pf.metadata.num_rows if pf.metadata is not None else None\n",
    "\n",
    "    batch_idx = 0\n",
    "    written = 0\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for batch in pf.iter_batches(columns=[\"__index_level_0__\", \"ctx\"], batch_size=batch_size, use_pandas_metadata=True):\n",
    "            batch_idx += 1\n",
    "\n",
    "            df_batch = batch.to_pandas()\n",
    "\n",
    "            df_batch = df_batch.reset_index()\n",
    "            df_batch.rename(\n",
    "                    columns={\"index\": \"row_id\"},\n",
    "                    inplace=True,\n",
    "            )\n",
    "\n",
    "            jsonl = df_batch.to_json(\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "                force_ascii=False\n",
    "            )\n",
    "\n",
    "            f_out.write(jsonl)\n",
    "            written += len(df_batch)\n",
    "\n",
    "            if total_rows is not None:\n",
    "                print(f\"batch {batch_idx}: {written}/{total_rows} строк\")\n",
    "            else:\n",
    "                print(f\"batch {batch_idx}: +{len(df_batch)} строк\")\n",
    "\n",
    "    print(f\"Готово, всего записано {written} строк в {out_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: 2000/48000 строк\n",
      "batch 2: 4000/48000 строк\n",
      "batch 3: 6000/48000 строк\n",
      "batch 4: 8000/48000 строк\n",
      "batch 5: 10000/48000 строк\n",
      "batch 6: 12000/48000 строк\n",
      "batch 7: 14000/48000 строк\n",
      "batch 8: 16000/48000 строк\n",
      "batch 9: 18000/48000 строк\n",
      "batch 10: 20000/48000 строк\n",
      "batch 11: 22000/48000 строк\n",
      "batch 12: 24000/48000 строк\n",
      "batch 13: 26000/48000 строк\n",
      "batch 14: 28000/48000 строк\n",
      "batch 15: 30000/48000 строк\n",
      "batch 16: 32000/48000 строк\n",
      "batch 17: 34000/48000 строк\n",
      "batch 18: 36000/48000 строк\n",
      "batch 19: 38000/48000 строк\n",
      "batch 20: 40000/48000 строк\n",
      "batch 21: 42000/48000 строк\n",
      "batch 22: 44000/48000 строк\n",
      "batch 23: 46000/48000 строк\n",
      "batch 24: 48000/48000 строк\n",
      "Готово, всего записано 48000 строк в third_part.jsonl\n"
     ]
    }
   ],
   "source": [
    "export_ctx_parquet_to_jsonl(\"third_part_normalize_source_code.parquet\", \"third_part.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4a78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_ids(ctx_jsonl_path: str | Path) -> list[int]:\n",
    "    ids = []\n",
    "    ctx_jsonl_path = Path(ctx_jsonl_path)\n",
    "\n",
    "    with open(ctx_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            row_id = rec[\"row_id\"]\n",
    "            ids.append(row_id)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5bb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ctx_effectively_empty(obj) -> bool:\n",
    "    if obj is None:\n",
    "        return True\n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return True\n",
    "        return all(is_ctx_effectively_empty(v) for v in obj.values())\n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return True\n",
    "        return all(is_ctx_effectively_empty(v) for v in obj)\n",
    "    return False\n",
    "\n",
    "def filter_jsonl(in_path: str | Path, out_path: str | Path) -> tuple[list[int], list[int]]:\n",
    "    in_path = Path(in_path)\n",
    "    out_path = Path(out_path)\n",
    "\n",
    "    total = 0\n",
    "    kept = 0\n",
    "    dropped = 0\n",
    "    bad_ids = []\n",
    "    good_ids = []\n",
    "\n",
    "    with in_path.open(\"r\", encoding=\"utf-8\") as fin, out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            total += 1\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            ctx = obj.get(\"ctx\")\n",
    "\n",
    "            if is_ctx_effectively_empty(ctx):\n",
    "                dropped += 1\n",
    "                bad_ids += [obj.get(\"row_id\")]\n",
    "                continue\n",
    "\n",
    "            # if \"__index_level_0__\" in obj:\n",
    "            #     row_id_val = obj.pop(\"__index_level_0__\")\n",
    "            #     obj.setdefault(\"row_id\", row_id_val)\n",
    "\n",
    "            fout.write(json.dumps(obj, ensure_ascii=False))\n",
    "            fout.write(\"\\n\")\n",
    "            kept += 1\n",
    "            good_ids += [obj.get(\"row_id\")]\n",
    "    print(f\"Всего строк: {total}, сохранено: {kept}, удалено пустых ctx: {dropped}\")\n",
    "    return good_ids, bad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdee5ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 66277, сохранено: 66267, удалено пустых ctx: 10\n"
     ]
    }
   ],
   "source": [
    "good_ids, bad_ids = filter_jsonl(CTX_JSONLS_DIR / \"third_part_without_macros.jsonl\", CTX_JSONLS_DIR / \"third_part_without_macros_clean.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d653844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_ctx_chunked(df: pd.DataFrame, ctx_jsonl_path: str | Path, ctx_column: str = \"ctx\", chunksize: int = 10_000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Читает JSONL батчами и заполняет колонку df[ctx_column]\n",
    "    по индексам row_id.\n",
    "    \"\"\"\n",
    "    ctx_jsonl_path = Path(ctx_jsonl_path)\n",
    "\n",
    "    # Если колонки пока нет — создаём\n",
    "    if ctx_column not in df.columns:\n",
    "        df[ctx_column] = None\n",
    "\n",
    "    # Читаем JSONL порциями\n",
    "    reader = pd.read_json(\n",
    "        ctx_jsonl_path,\n",
    "        lines=True,\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    total_chunks = 0\n",
    "    for chunk in reader:\n",
    "        total_chunks += 1\n",
    "        if \"row_id\" not in chunk.columns:\n",
    "            raise ValueError(\"В JSONL нет колонки 'row_id'\")\n",
    "\n",
    "        # Ставим row_id как индекс\n",
    "        chunk = chunk.set_index(\"row_id\")\n",
    "\n",
    "        # Заполняем df по индексам\n",
    "        # Предполагается, что индексы df совпадают с row_id\n",
    "        df.loc[chunk.index, ctx_column] = chunk[ctx_column]\n",
    "\n",
    "        print(f\"Обработан chunk {total_chunks}, строк: {len(chunk)}\")\n",
    "\n",
    "    print(f\"Готово, обработано chunk'ов: {total_chunks}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7feb30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_jsonl_files(in_paths, out_path: str | Path, check_duplicates: bool = True) -> None:\n",
    "    out_path = Path(out_path)\n",
    "    seen_ids = set()\n",
    "\n",
    "    total_in = 0\n",
    "    total_out = 0\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for p in in_paths:\n",
    "            p = Path(p)\n",
    "            print(f\"Обрабатываем файл: {p}\")\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    total_in += 1\n",
    "                    obj = json.loads(line)\n",
    "\n",
    "                    if \"row_id\" not in obj:\n",
    "                        raise ValueError(f\"В файле {p} объект без 'row_id': {obj}\")\n",
    "                    if \"ctx\" not in obj:\n",
    "                        raise ValueError(f\"В файле {p} объект без 'ctx': {obj}\")\n",
    "\n",
    "                    row_id = obj[\"row_id\"]\n",
    "\n",
    "                    if check_duplicates:\n",
    "                        if row_id in seen_ids:\n",
    "                            raise ValueError(\n",
    "                                f\"Дубликат row_id={row_id!r} в файле {p}. \"\n",
    "                                \"Ожидалось, что row_id не повторяются.\"\n",
    "                            )\n",
    "                        seen_ids.add(row_id)\n",
    "\n",
    "                    fout.write(json.dumps(obj, ensure_ascii=False))\n",
    "                    fout.write(\"\\n\")\n",
    "                    total_out += 1\n",
    "\n",
    "    print(\n",
    "        f\"Готово. Прочитано строк: {total_in}, \"\n",
    "        f\"записано в {out_path}: {total_out}. \"\n",
    "        f\"Уникальных row_id: {len(seen_ids) if check_duplicates else 'не проверяли'}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18e2ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обрабатываем файл: C:\\Users\\olegk\\Downloads\\Telegram Desktop\\Лабы\\Технологии проектирования ПО\\Project\\notebooks\\ctx_jsonls\\first_part.jsonl\n",
      "Обрабатываем файл: C:\\Users\\olegk\\Downloads\\Telegram Desktop\\Лабы\\Технологии проектирования ПО\\Project\\notebooks\\ctx_jsonls\\second_part.jsonl\n",
      "Обрабатываем файл: C:\\Users\\olegk\\Downloads\\Telegram Desktop\\Лабы\\Технологии проектирования ПО\\Project\\notebooks\\ctx_jsonls\\third_part.jsonl\n",
      "Обрабатываем файл: C:\\Users\\olegk\\Downloads\\Telegram Desktop\\Лабы\\Технологии проектирования ПО\\Project\\notebooks\\ctx_jsonls\\fourth_part.jsonl\n",
      "Готово. Прочитано строк: 71400, записано в C:\\Users\\olegk\\Downloads\\Telegram Desktop\\Лабы\\Технологии проектирования ПО\\Project\\notebooks\\ctx_jsonls\\data_with_macros.jsonl: 71400. Уникальных row_id: 71400\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    CTX_JSONLS_DIR / \"first_part.jsonl\",\n",
    "    CTX_JSONLS_DIR / \"second_part.jsonl\",\n",
    "    CTX_JSONLS_DIR / \"third_part.jsonl\",\n",
    "    CTX_JSONLS_DIR / \"fourth_part.jsonl\",\n",
    "]\n",
    "merge_jsonl_files(files, CTX_JSONLS_DIR / \"data_with_macros.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf502fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_utf8_text(x):\n",
    "    # если вдруг bytes -> пытаемся прочитать как utf-8, при проблемах заменяем\n",
    "    if isinstance(x, (bytes, bytearray)):\n",
    "        return x.decode(\"utf-8\", errors=\"replace\")\n",
    "    # если str, но при кодировании в utf-8 вылезают суррогаты -> заменяем проблемные места\n",
    "    try:\n",
    "        x.encode(\"utf-8\")\n",
    "        return x\n",
    "    except UnicodeEncodeError:\n",
    "        return x.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fd876a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_need_macros[\"source_code\"] = remain_need_macros[\"source_code\"].map(ensure_utf8_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4305c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def compute_ctx_worker(args : tuple[str, int, int]) -> dict:\n",
    "    source_code, error_line, radius = args\n",
    "    try:\n",
    "        return safe_extract_context(source_code, error_line, False, radius)\n",
    "    except Exception as e:\n",
    "        print(f\"error, ctx_failed: {e!r}, line: {error_line}\")\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def add_ctx_parallel_to_jsonl(df : pd.DataFrame, radius: int, out_jsonl : str | Path, max_workers : int = 4, batch_size : int = 1000) -> None:\n",
    "    out_jsonl = Path(out_jsonl)\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        print(\"DataFrame пустой\")\n",
    "        return\n",
    "    \n",
    "    row_index = df.index.to_numpy(copy=False)\n",
    "    source_values = df[\"source_code\"].to_numpy(copy=False)\n",
    "    line_values = df[\"error_line\"].to_numpy(copy=False)\n",
    "\n",
    "    total_batches = math.ceil(n / batch_size)\n",
    "    print(f\"Всего строк: {n}, батчей: {total_batches}, max_workers: {max_workers}, batch_size: {batch_size}\")\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            for b_idx, start in enumerate(range(0, n, batch_size), start=1):\n",
    "                stop = min(start + batch_size, n)\n",
    "\n",
    "                batch_row_ids = row_index[start:stop]\n",
    "                batch_sources = source_values[start:stop]\n",
    "                batch_lines = line_values[start:stop]\n",
    "\n",
    "                args_iter = ((s, l, radius) for s, l in zip(batch_sources, batch_lines))\n",
    "                results = list(ex.map(compute_ctx_worker, args_iter))\n",
    "\n",
    "                df_batch = pd.DataFrame({\n",
    "                        \"row_id\": batch_row_ids,\n",
    "                        \"ctx\": results,\n",
    "                    })\n",
    "                json_str = df_batch.to_json(\n",
    "                    orient=\"records\",\n",
    "                    lines=True,\n",
    "                    force_ascii=False,\n",
    "                )\n",
    "                f_out.write(json_str)\n",
    "                print(f\"[{b_idx}/{total_batches}] обработано строк: {stop}/{n}\")\n",
    "\n",
    "    print(\"Обработка всех батчей завершена\")\n",
    "    return\n",
    "\n",
    "\n",
    "def add_ctx_sequential_to_jsonl(df: pd.DataFrame, radius: int, out_jsonl : str | Path, batch_size: int = 1000) -> None:\n",
    "    out_jsonl = Path(out_jsonl)\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return\n",
    "\n",
    "    row_index = df.index.to_numpy(copy=False)\n",
    "    source_values = df[\"source_code\"].to_numpy(copy=False)\n",
    "    line_values = df[\"error_line\"].to_numpy(copy=False)\n",
    "\n",
    "    total_batches = math.ceil(n / batch_size)\n",
    "    print(f\"Всего строк: {n}, батчей: {total_batches}, batch_size: {batch_size} (SEQUENTIAL)\")\n",
    "\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        for b_idx, start in enumerate(range(0, n, batch_size), start=1):\n",
    "            stop = min(start + batch_size, n)\n",
    "\n",
    "            batch_row_ids = row_index[start:stop]\n",
    "            batch_sources = source_values[start:stop]\n",
    "            batch_lines = line_values[start:stop]\n",
    "\n",
    "            results = []\n",
    "            for src, line in zip(batch_sources, batch_lines):\n",
    "                ctx = compute_ctx_worker((src, line, radius))\n",
    "                results.append(ctx)\n",
    "\n",
    "            df_batch = pd.DataFrame({\n",
    "                    \"row_id\": batch_row_ids,\n",
    "                    \"ctx\": results,\n",
    "                })\n",
    "            json_str = df_batch.to_json(\n",
    "                orient=\"records\",\n",
    "                lines=True,\n",
    "                force_ascii=False,\n",
    "            )\n",
    "            f_out.write(json_str)\n",
    "            print(f\"[{b_idx}/{total_batches}] обработано строк: {stop}/{n}\")\n",
    "\n",
    "    print(\"Обработка всех батчей (последовательно) завершена\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ctx_sequential_to_jsonl(need_without_macros, 2, CTX_JSONLS_DIR / \"first_part_without_macros.jsonl\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cec842",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ctx_parallel_to_jsonl(remain_need, 2, CTX_JSONLS_DIR / \"third_part_without_macros.jsonl\", 3, 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
